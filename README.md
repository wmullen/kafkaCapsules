# Warning, this code is heavily in development and will change on a whim.  Things will break.

# Introduction
This code is an initial attempt at creating a cohesive way for representing a basic
Data Capsule in Kafka using python 3.  It is heavily inspired by Nitesh's quickstart files
and Eric Chen's hackathon starter code.  



The basic usage is as follows.  Upon creating the object, the user passes in a public writer (or signature) key.  If this is a new capsule, then the user calls `createCapsule`.  If this public writer key is already being used, then the user calls `loadCapsule` to populate the object with the capsule's information.  This will later be changed to support multiple capsules per key.  

Check main.py for a simple example.  

**Author:** William Mullen

# Using the Capsules
## Setup
1. Install [Docker](https://docs.docker.com/get-docker/).
2. If not on Mac or Windows, install [Docker Compose](https://docs.docker.com/compose/install/).
3. Install [Python 3](https://www.python.org/downloads/).
4. Run `pip install kafka-python`.

## Running Code
1. Run `start-kafka.sh`.
2. Run your python file.

## Teardown

In the event something goes horribly wrong with Kafka, following this section, deleting the containers, and then running `start-kafka.sh` will pull a fresh copy of the Kafka containers.  The steps detailed here are mainly for users running on Linux, as Docker Desktop will handle these operations through the Dashboard.  The process is generally the same, find the containers, stop them, and then delete if desired.

1. Run `docker ps` and find the names associated with the `confluentinc/cp-kafka` and `confluent/zookeeper containers`.  If you are using Docker Desktop, simply navigate to your Dashboard.
2. Run `docker stop [container name]` using the names you found in step 1. 
3. If desired, run `docker rm [container name]` on both containers.  WARNING: This will wipe any capsules you have created.  Only do this step if you want a fresh copy of Kafka or are done with Data Capsules.  

# KafkaCapsule Documentation
A KafkaCapsule emulates a Data Capsule as originally described using two Kafka topics.

TODO: Add image of structure and description

## KafkaCapsule Attributes
A KafkaCapsule has the following attributes:

1. `gdpName`
    * A 256 bit name that identifies this KafkaCapsule.  Currently a sha256 hash of the `pubWriterKey`. 
2. `dataTopic`
    * An internally used name for the data Kafka topic (see image above).  Generated by adding `_data` to the end of the `gdpName`.  
3. `hashTopic`
    * An internally used name for the hash Kafka topic (see image above).  Generated by adding `_hash` to the end of the `gdpName`.  
4. `pubWriterKey`
    * A stored version of the public signature key of the owner of this capsule.
5. `self.admin_client`
    * An internal client used to create the two topics.
6. `topic_name`
    * Internal human readable name retained for debugging purposes.  This is not stored in Kafka.
7. `self.ready`
    * This flag is initialized to False but is set to True upon successful call to `createCapsule` or `loadCapsule`.  Indicates whether a capsule is ready to be read from or written to.  There is almost certainly a better way to do this check.    

## KafkaCapsule Functions
### `createCapsule(self)`
Creates a new capusle based on a hash of the object's `pubWriterkey`.  First checks to see if a capsule already exists with this name.  If not, then two new Kafka topics are spawned, one for the capsule's data and another for its hashes.  Once successfully finished, sets the `ready` flag to true and prints a notification.  Returns the capsule's `gdpName` upon success.

### `loadCapsule(self, gdpName)`
Loads the information for a capsule with name `gdpName` into the object.  This populates the `gdpName`, `dataTopic`, `hashTopic`, and `pubWriterKey` fields.  Once successfully finished, sets the `ready` flag to true and prints a notification.  This function does not return anything.

### `append(self, data)`
Appends `data` to the end of the capsule represented by this object.  Under the hood, this function appends the data to the `dataTopic` and the data's hash to the `hashTopic`.  Assumes data is already a bytestring. This function prints when the append is complete and does not return anything.

### `read(self, offset=0)`
Reads the data stored in the capsule represented by this object and returns the byte strings in a list.  The offset is not yet implemented but will eventually allow for reading starting at a certain record.  Currently does not verify hashes.

### `readLast(self)`
Not yet implemented.  Will eventually return only the more recent record in the capsule.  

### `subscribe(self)`
Not yet implemented.  Will eventually allow for a subscription to a capsule.  

### `getName(self)`
Returns the `gdpName` stored in this object.  

# Next Steps
- [ ] Update KafkaCapsule description with strucutre from Google Doc
- [x] Add hash generation for appends
- [ ] Add verify function to verify hashes
- [ ] Change metadata to bytearray to hold more items
- [ ] Figure out how to use the seek function - TopicPartition namedtuple?
- [ ] readLast function
- [ ] subscribe function
- [ ] Automated tests
- [ ] Add docstrings to functions
- [ ] Change `gdpName` to include capsule name to prevent only one capsule permitted per `pubWriterKey`

# Questions
1. What fields absolutely need to be in the capsule from the start?
2. What verification steps should be done? Reader and writer permissions?